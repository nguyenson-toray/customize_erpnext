#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import frappe
import pandas as pd
from frappe.utils import nowdate, flt, cstr, get_site_path
from collections import defaultdict
import os
import datetime
import sys
import json

# Company m·∫∑c ƒë·ªãnh
DEFAULT_COMPANY = "Toray International, VietNam Company Limited - Quang Ngai Branch"

# ƒê∆∞·ªùng d·∫´n file m·∫∑c ƒë·ªãnh
DEFAULT_FILE_PATH = "/home/sonnt/frappe-bench/sites/erp-sonnt.tiqn.local/private/files/create_material_receipt.xlsx"

class Logger:
    """Class ƒë·ªÉ ghi log v√†o file v√† console"""
    def __init__(self, log_file_path=None):
        if not log_file_path:
            # T·∫°o log file ·ªü c√πng th∆∞ m·ª•c v·ªõi script
            script_dir = os.path.dirname(os.path.abspath(__file__))
            log_file_path = os.path.join(script_dir, "create_material_receipt.txt")
        
        self.log_file_path = log_file_path
        self.start_time = datetime.datetime.now()
        
        # Kh·ªüi t·∫°o log file
        self._write_header()
    
    def _write_header(self):
        """Vi·∫øt header cho log file"""
        header = f"""
{'='*80}
MATERIAL RECEIPT BULK CREATION LOG
Started at: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}
Script: {__file__}
{'='*80}

"""
        with open(self.log_file_path, 'w', encoding='utf-8') as f:
            f.write(header)
    
    def log(self, message, level="INFO", print_console=True):
        """Ghi log v√†o file v√† console"""
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_line = f"[{timestamp}] [{level}] {message}"
        
        # Ghi v√†o file
        try:
            with open(self.log_file_path, 'a', encoding='utf-8') as f:
                f.write(log_line + '\n')
        except Exception as e:
            print(f"Error writing to log file: {e}")
        
        # In ra console n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if print_console:
            print(message)
    
    def log_success(self, message, print_console=True):
        """Log th√†nh c√¥ng"""
        self.log(f" {message}", "SUCCESS", print_console)
    
    def log_error(self, message, print_console=True):
        """Log l·ªói"""
        self.log(f"‚ùå {message}", "ERROR", print_console)
    
    def log_warning(self, message, print_console=True):
        """Log c·∫£nh b√°o"""
        self.log(f" {message}", "WARNING", print_console)
    
    def log_info(self, message, print_console=True):
        """Log th√¥ng tin"""
        self.log(f"üìã {message}", "INFO", print_console)
    
    def log_separator(self, title="", print_console=True):
        """Log d√≤ng ph√¢n c√°ch"""
        if title:
            separator = f"\n{'='*20} {title} {'='*20}"
        else:
            separator = "="*60
        self.log(separator, "INFO", print_console)
    
    def finalize(self, result):
        """K·∫øt th√∫c log v·ªõi t√≥m t·∫Øt"""
        end_time = datetime.datetime.now()
        duration = end_time - self.start_time
        
        footer = f"""

{'='*80}
EXECUTION COMPLETED
End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}
Duration: {duration}
Success Count: {result.get('success_count', 0)}
Error Count: {result.get('error_count', 0)}
Total Items: {result.get('total_items', 0)}
Log file: {self.log_file_path}
{'='*80}
"""
        with open(self.log_file_path, 'a', encoding='utf-8') as f:
            f.write(footer)
        
        print(f"\nüìÅ Log ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {self.log_file_path}")

# Global logger instance
logger = None

@frappe.whitelist()
def validate_excel_file(file_url):
    """Performance optimized Excel validation for Material Receipt"""
    global logger
    
    try:
        # Clear caches for validation
        global item_cache, warehouse_cache
        item_cache.clear()
        warehouse_cache.clear()
        
        # Kh·ªüi t·∫°o logger
        logger = Logger()
        
        logger.log_separator("B·∫ÆT ƒê·∫¶U PERFORMANCE OPTIMIZED VALIDATION")
        logger.log_info(f"File URL: {file_url}")
        
        # Chuy·ªÉn ƒë·ªïi file URL th√†nh ƒë∆∞·ªùng d·∫´n th·ª±c
        file_path = get_file_path_from_url(file_url)
        
        if not os.path.exists(file_path):
            logger.log_error(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")
            return {
                "success": False,
                "message": f"File kh√¥ng t·ªìn t·∫°i: {file_path}",
                "log_file_path": logger.log_file_path
            }
        
        # ƒê·ªçc file Excel
        logger.log_info("ƒêang ƒë·ªçc file Excel...")
        df = pd.read_excel(file_path)
        
        logger.log_info(f"ƒê√£ ƒë·ªçc ƒë∆∞·ª£c {len(df)} d√≤ng d·ªØ li·ªáu")
        
        # Validate columns
        validation_result = validate_excel_columns(df)
        if not validation_result["success"]:
            logger.log_error(validation_result["message"])
            return {
                "success": False,
                "message": validation_result["message"],
                "log_file_path": logger.log_file_path
            }
        
        # Clean data
        df = clean_dataframe(df)
        
        # Performance optimized detailed validation
        logger.log_info("Starting batch validation with performance optimizations...")
        detailed_validation = validate_excel_data_detailed(df)
        
        logger.log_separator("K·∫æT QU·∫¢ OPTIMIZED VALIDATION")
        
        if detailed_validation["success"]:
            logger.log_success(f"Validation th√†nh c√¥ng!")
            logger.log_info(f"Total rows: {detailed_validation['total_rows']}")
            logger.log_info(f"Valid rows: {detailed_validation['valid_rows']}")
            logger.log_info(f"Groups: {detailed_validation['groups_count']}")
            logger.log_info(f"Cached items: {len(item_cache)}")
            logger.log_info(f"Cached warehouses: {len(warehouse_cache)}")
        else:
            logger.log_error("Validation th·∫•t b·∫°i!")
            for error in detailed_validation.get("errors", []):
                logger.log_error(error)
        
        # Finalize log
        logger.finalize({
            "success_count": 1 if detailed_validation["success"] else 0,
            "error_count": 0 if detailed_validation["success"] else 1,
            "total_items": len(df)
        })
        
        detailed_validation["log_file_path"] = logger.log_file_path
        return detailed_validation
        
    except Exception as e:
        error_msg = f"L·ªói validation: {str(e)}"
        if logger:
            logger.log_error(error_msg)
            logger.finalize({
                "success_count": 0,
                "error_count": 1,
                "total_items": 0
            })
        
        return {
            "success": False,
            "message": error_msg,
            "log_file_path": logger.log_file_path if logger else None
        }

@frappe.whitelist()
def import_material_receipt_from_excel(file_url):
    """Import Material Receipt t·ª´ Excel file"""
    global logger
    
    try:
        # Kh·ªüi t·∫°o logger
        logger = Logger()
        
        logger.log_separator("B·∫ÆT ƒê·∫¶U IMPORT MATERIAL RECEIPT")
        
        # Chuy·ªÉn ƒë·ªïi file URL th√†nh ƒë∆∞·ªùng d·∫´n th·ª±c
        file_path = get_file_path_from_url(file_url)
        
        # G·ªçi h√†m create_material_receipt hi·ªán t·∫°i
        result = create_material_receipt(file_path)
        
        return result
        
    except Exception as e:
        error_msg = f"L·ªói import: {str(e)}"
        if logger:
            logger.log_error(error_msg)
        
        return {
            "success": False,
            "message": error_msg,
            "success_count": 0,
            "error_count": 1,
            "total_items": 0,
            "created_entries": [],
            "errors": [error_msg]
        }

def get_file_path_from_url(file_url):
    """Chuy·ªÉn ƒë·ªïi file URL th√†nh ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø"""
    if file_url.startswith('/files/'):
        site_path = get_site_path()
        return os.path.join(site_path, "public", file_url.lstrip('/'))
    elif file_url.startswith('/private/files/'):
        site_path = get_site_path()
        return os.path.join(site_path, file_url.lstrip('/'))
    else:
        # Assume it's already a full path
        return file_url

def validate_excel_data_detailed(df):
    """Performance optimized validation v·ªõi batch processing"""
    result = {
        "success": True,
        "total_rows": len(df),
        "valid_rows": 0,
        "groups_count": 0,
        "validation_details": {
            "missing_items": [],
            "missing_warehouses": [],
            "invoice_issues": [],
            "errors": [],
            "suggestions": []
        }
    }
    
    try:
        # Group theo custom_no
        grouped_data = df.groupby('custom_no')
        result["groups_count"] = len(grouped_data)
        
        logger.log_info(f"T√¨m th·∫•y {len(grouped_data)} nh√≥m custom_no")
        
        # Performance optimization: Batch lookup items and warehouses
        unique_patterns = df['custom_item_name_detail'].dropna().unique().tolist()
        logger.log_info(f"Batch lookup for {len(unique_patterns)} unique items...")
        
        # Batch find all items
        items_map = batch_find_items(unique_patterns)
        
        # Get all valid item codes for warehouse lookup
        valid_item_codes = [item['item_code'] for item in items_map.values() if item is not None]
        
        # Batch find all warehouses
        logger.log_info(f"Batch lookup for {len(valid_item_codes)} warehouses...")
        warehouses_map = batch_get_warehouses(valid_item_codes)
        
        # Batch find suggestions for missing items
        missing_patterns = [pattern for pattern, item in items_map.items() if item is None]
        suggestions_map = {}
        if missing_patterns:
            logger.log_info(f"Finding suggestions for {len(missing_patterns)} missing items...")
            for pattern in missing_patterns:
                suggestions_map[pattern] = find_similar_items(pattern, limit=3)
        
        valid_row_count = 0
        
        for group_index, (custom_no, group_df) in enumerate(grouped_data, 1):
            logger.log_info(f"Validating nh√≥m {group_index}/{len(grouped_data)}: {custom_no}")
            
            for index, row in group_df.iterrows():
                row_number = index + 2
                
                try:
                    # Use cached item lookup
                    pattern_search = row.get('custom_item_name_detail', '')
                    item = items_map.get(pattern_search)
                    
                    if not item:
                        suggestions = suggestions_map.get(pattern_search, [])
                        
                        missing_item_info = {
                            "custom_item_name_detail": pattern_search,
                            "row": row_number,
                            "suggestions": suggestions[:3]
                        }
                        
                        result["validation_details"]["missing_items"].append(missing_item_info)
                        result["success"] = False
                        continue
                    
                    # Check warehouse - priority: Excel > Default warehouse
                    warehouse = None
                    excel_warehouse = row.get('t_warehouse', '')
                    
                    if excel_warehouse and pd.notna(excel_warehouse) and str(excel_warehouse).strip():
                        # Validate warehouse t·ª´ Excel c√≥ t·ªìn t·∫°i kh√¥ng
                        excel_warehouse = str(excel_warehouse).strip()
                        if frappe.db.exists("Warehouse", excel_warehouse):
                            warehouse = excel_warehouse
                            logger.log_info(f"  Using warehouse from Excel: {warehouse} for item {item['item_code']}")
                        else:
                            result["validation_details"]["missing_warehouses"].append({
                                "item_code": item['item_code'],
                                "warehouse": f"Excel warehouse '{excel_warehouse}' not found",
                                "row": row_number
                            })
                            result["success"] = False
                            continue
                    else:
                        # Fallback to default warehouse
                        warehouse = warehouses_map.get(item['item_code'])
                        if warehouse:
                            logger.log_info(f"  Using default warehouse: {warehouse} for item {item['item_code']}")
                    
                    if not warehouse:
                        result["validation_details"]["missing_warehouses"].append({
                            "item_code": item['item_code'],
                            "warehouse": "No warehouse specified and no default warehouse",
                            "row": row_number
                        })
                        result["success"] = False
                        continue
                    
                    # Validate qty
                    qty = flt(row.get('qty', 0))
                    if qty <= 0:
                        result["validation_details"]["errors"].append(
                            f"Row {row_number}: Invalid quantity {qty} for item {item['item_code']}"
                        )
                        result["success"] = False
                        continue
                    
                    # Validate custom_invoice_number
                    invoice_number = cstr(row.get('custom_invoice_number', '')).strip()
                    if not invoice_number:
                        result["validation_details"]["errors"].append(
                            f"Row {row_number}: Missing invoice number for item {item['item_code']}"
                        )
                        result["success"] = False
                        continue
                    
                    valid_row_count += 1
                    
                except Exception as e:
                    result["validation_details"]["errors"].append(
                        f"Row {row_number}: {str(e)}"
                    )
                    result["success"] = False
                    continue
        
        result["valid_rows"] = valid_row_count
        
        # Log summary
        logger.log_info(f"Validation summary:")
        logger.log_info(f"  - Total rows: {result['total_rows']}")
        logger.log_info(f"  - Valid rows: {result['valid_rows']}")
        logger.log_info(f"  - Missing items: {len(result['validation_details']['missing_items'])}")
        logger.log_info(f"  - Missing warehouses: {len(result['validation_details']['missing_warehouses'])}")
        logger.log_info(f"  - Invoice issues: {len(result['validation_details']['invoice_issues'])}")
        logger.log_info(f"  - General errors: {len(result['validation_details']['errors'])}")
        
        return result
        
    except Exception as e:
        result["success"] = False
        result["validation_details"]["errors"].append(f"Validation error: {str(e)}")
        logger.log_error(f"Detailed validation error: {str(e)}")
        return result

def find_similar_items(search_term, limit=5):
    """
    T√¨m items c√≥ custom_item_name_detail t∆∞∆°ng t·ª± ƒë·ªÉ suggest khi exact match th·∫•t b·∫°i
    """
    try:
        if not search_term or pd.isna(search_term):
            return []
            
        search_term = cstr(search_term).strip()
        
        # T√¨m items c√≥ custom_item_name_detail ch·ª©a m·ªôt ph·∫ßn c·ªßa search_term
        similar_items = frappe.db.sql("""
            SELECT 
                item_code,
                custom_item_name_detail,
                CASE 
                    WHEN custom_item_name_detail = %s THEN 100
                    WHEN custom_item_name_detail LIKE %s THEN 90
                    WHEN custom_item_name_detail LIKE %s THEN 80
                    WHEN custom_item_name_detail LIKE %s THEN 70
                    ELSE 60
                END as similarity_score
            FROM `tabItem`
            WHERE custom_item_name_detail IS NOT NULL
            AND custom_item_name_detail != ''
            AND has_variants = 0
            AND (
                custom_item_name_detail LIKE %s
                OR custom_item_name_detail LIKE %s
                OR custom_item_name_detail LIKE %s
            )
            ORDER BY similarity_score DESC, custom_item_name_detail
            LIMIT %s
        """, (
            search_term,  # Exact match (shouldn't happen here but for completeness)
            f"{search_term}%",  # Starts with
            f"%{search_term}%",  # Contains
            f"%{search_term}",   # Ends with
            f"%{search_term}%",  # Contains (repeat for OR)
            f"{search_term}%",   # Starts with (repeat for OR)
            f"%{search_term}",   # Ends with (repeat for OR)
            limit
        ), as_dict=True)
        
        return similar_items
        
    except Exception as e:
        if logger:
            logger.log_error(f"Error finding similar items for '{search_term}': {str(e)}")
        return []

def get_default_warehouse_for_item(item_code):
    """L·∫•y default warehouse cho item t·ª´ Item Default v·ªõi caching"""
    try:
        # Check cache first
        if item_code in warehouse_cache:
            return warehouse_cache[item_code]
        
        company = DEFAULT_COMPANY
        
        # Query Item Default table
        default_warehouse = frappe.db.sql("""
            SELECT default_warehouse
            FROM `tabItem Default`
            WHERE parent = %s AND company = %s
            LIMIT 1
        """, (item_code, company), as_dict=True)
        
        warehouse = None
        if default_warehouse and default_warehouse[0].default_warehouse:
            warehouse = default_warehouse[0].default_warehouse
        else:
            # Fallback: get any warehouse for the company
            fallback_warehouse = frappe.db.sql("""
                SELECT name
                FROM `tabWarehouse`
                WHERE company = %s AND is_group = 0
                LIMIT 1
            """, company, as_dict=True)
            
            if fallback_warehouse:
                warehouse = fallback_warehouse[0].name
                logger.log_warning(f"No default warehouse for {item_code}, using fallback: {warehouse}")
        
        # Cache the result (including None)
        warehouse_cache[item_code] = warehouse
        return warehouse
        
    except Exception as e:
        logger.log_error(f"Error getting default warehouse for {item_code}: {str(e)}")
        warehouse_cache[item_code] = None
        return None

def batch_get_warehouses(item_codes):
    """
    Performance optimization: Batch lookup for warehouses
    """
    try:
        if not item_codes:
            return {}
        
        # Filter out cached items
        uncached_codes = [code for code in item_codes if code not in warehouse_cache]
        
        if uncached_codes:
            company = DEFAULT_COMPANY
            
            # Batch query for all uncached warehouses
            warehouses = frappe.db.sql("""
                SELECT parent as item_code, default_warehouse
                FROM `tabItem Default`
                WHERE parent IN %(item_codes)s AND company = %(company)s
            """, {"item_codes": uncached_codes, "company": company}, as_dict=True)
            
            # Update cache with results
            found_codes = set()
            for wh in warehouses:
                if wh['default_warehouse']:
                    warehouse_cache[wh['item_code']] = wh['default_warehouse']
                    found_codes.add(wh['item_code'])
            
            # For items without default warehouse, get fallback
            unfound_codes = [code for code in uncached_codes if code not in found_codes]
            if unfound_codes:
                # Get fallback warehouse once
                fallback = frappe.db.sql("""
                    SELECT name
                    FROM `tabWarehouse`
                    WHERE company = %s AND is_group = 0
                    LIMIT 1
                """, company, as_dict=True)
                
                fallback_warehouse = fallback[0].name if fallback else None
                
                # Cache fallback for all unfound codes
                for code in unfound_codes:
                    warehouse_cache[code] = fallback_warehouse
                    if fallback_warehouse:
                        logger.log_warning(f"No default warehouse for {code}, using fallback: {fallback_warehouse}")
        
        # Return all requested warehouses from cache
        return {code: warehouse_cache.get(code) for code in item_codes}
        
    except Exception as e:
        if logger:
            logger.log_error(f"Error in batch warehouse lookup: {str(e)}")
        return {}

def create_material_receipt(file_path=None):
    """
    H√†m ch√≠nh ƒë·ªÉ t·∫°o Material Receipt Stock Entry t·ª´ file Excel
    
    Args:
        file_path (str, optional): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file Excel. 
                                 N·∫øu None, s·∫Ω d√πng ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh.
    
    Returns:
        dict: K·∫øt qu·∫£ th·ª±c hi·ªán
    """
    global logger
    
    try:
        # Kh·ªüi t·∫°o logger
        logger = Logger()
        
        logger.log_separator("B·∫ÆT ƒê·∫¶U T·∫†O MATERIAL RECEIPT STOCK ENTRIES")
        
        # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n file
        if not file_path:
            file_path = DEFAULT_FILE_PATH
            logger.log_info(f"S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n m·∫∑c ƒë·ªãnh: {file_path}")
        else:
            logger.log_info(f"S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n: {file_path}")
        
        # Validate tr∆∞·ªõc khi ch·∫°y
        logger.log_info("B·∫Øt ƒë·∫ßu validation...")
        validation_result = validate_before_creation(file_path)
        if not validation_result["success"]:
            logger.log_error(f"Validation th·∫•t b·∫°i: {validation_result['message']}")
            logger.finalize(validation_result)
            return validation_result
        
        logger.log_success("Validation th√†nh c√¥ng")
        
        # ƒê·ªçc v√† x·ª≠ l√Ω file Excel
        logger.log_info("B·∫Øt ƒë·∫ßu x·ª≠ l√Ω file Excel...")
        result = process_excel_file(file_path)
        
        logger.log_separator("K·∫æT QU·∫¢ CU·ªêI C√ôNG")
        logger.log_success(f"T·∫°o th√†nh c√¥ng: {result['success_count']} Stock Entry")
        
        if result['error_count'] > 0:
            logger.log_error(f"L·ªói: {result['error_count']} nh√≥m")
        
        logger.log_info(f"T·ªïng items ƒë√£ x·ª≠ l√Ω: {result['total_items']}")
        
        # Log danh s√°ch Stock Entries ƒë√£ t·∫°o
        if result['created_entries']:
            logger.log_info("Danh s√°ch Stock Entries ƒë√£ t·∫°o:")
            for i, entry_name in enumerate(result['created_entries'], 1):
                logger.log_info(f"  {i}. {entry_name}")
        
        # Log danh s√°ch l·ªói n·∫øu c√≥
        if result['errors']:
            logger.log_warning("Danh s√°ch l·ªói:")
            for i, error in enumerate(result['errors'], 1):
                logger.log_error(f"  {i}. {error}")
        
        if result['success_count'] > 0:
            frappe.db.commit()
            logger.log_success("ƒê√£ commit th√†nh c√¥ng!")
        
        # Finalize log
        logger.finalize(result)
        
        return result
        
    except Exception as e:
        frappe.db.rollback()
        error_msg = f"L·ªói chung: {str(e)}"
        
        if logger:
            logger.log_error(error_msg)
            result = {
                "success": False,
                "message": error_msg,
                "success_count": 0,
                "error_count": 1,
                "total_items": 0,
                "created_entries": [],
                "errors": [error_msg]
            }
            logger.finalize(result)
        else:
            print(f"‚ùå {error_msg}")
        
        return {
            "success": False,
            "message": error_msg,
            "success_count": 0,
            "error_count": 1,
            "total_items": 0,
            "created_entries": [],
            "errors": [error_msg]
        }

def validate_before_creation(file_path):
    """
    Ki·ªÉm tra ƒëi·ªÅu ki·ªán tr∆∞·ªõc khi t·∫°o
    
    Args:
        file_path (str): ƒê∆∞·ªùng d·∫´n file Excel
        
    Returns:
        dict: K·∫øt qu·∫£ validation
    """
    try:
        # Ki·ªÉm tra file t·ªìn t·∫°i
        if not os.path.exists(file_path):
            message = f"File kh√¥ng t·ªìn t·∫°i: {file_path}"
            if logger:
                logger.log_error(message)
            return {"success": False, "message": message}
        
        # Ki·ªÉm tra company t·ªìn t·∫°i
        if not frappe.db.exists("Company", DEFAULT_COMPANY):
            message = f"Company kh√¥ng t·ªìn t·∫°i: {DEFAULT_COMPANY}"
            if logger:
                logger.log_error(message)
            return {"success": False, "message": message}
        
        # Ki·ªÉm tra file c√≥ ƒë·ªçc ƒë∆∞·ª£c kh√¥ng
        try:
            df = pd.read_excel(file_path, nrows=1)
            if logger:
                logger.log_success(f"File Excel h·ª£p l·ªá v·ªõi {len(df.columns)} c·ªôt")
        except Exception as e:
            message = f"Kh√¥ng th·ªÉ ƒë·ªçc file Excel: {str(e)}"
            if logger:
                logger.log_error(message)
            return {"success": False, "message": message}
        
        return {"success": True, "message": "Validation th√†nh c√¥ng"}
        
    except Exception as e:
        message = f"L·ªói validation: {str(e)}"
        if logger:
            logger.log_error(message)
        return {"success": False, "message": message}

def process_excel_file(file_path):
    """
    Performance optimized Excel processing v·ªõi batch operations
    """
    result = {
        "success": True,
        "message": "",
        "success_count": 0,
        "error_count": 0,
        "total_items": 0,
        "created_entries": [],
        "created_entries_details": [],  # New field for detailed information
        "errors": []
    }
    
    try:
        # Clear caches at start
        global item_cache, warehouse_cache
        item_cache.clear()
        warehouse_cache.clear()
        
        # ƒê·ªçc file Excel
        logger.log_info(f"ƒêang ƒë·ªçc file: {file_path}")
        df = pd.read_excel(file_path)
        
        logger.log_info(f"ƒê√£ ƒë·ªçc ƒë∆∞·ª£c {len(df)} d√≤ng d·ªØ li·ªáu")
        logger.log_info(f"C√°c c·ªôt: {list(df.columns)}")
        
        # Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc
        validation_result = validate_excel_columns(df)
        if not validation_result["success"]:
            result["success"] = False
            result["message"] = validation_result["message"]
            result["errors"].append(validation_result["message"])
            logger.log_error(validation_result["message"])
            return result
        
        logger.log_success("C√°c c·ªôt Excel h·ª£p l·ªá")
        
        # L√†m s·∫°ch d·ªØ li·ªáu
        df = clean_dataframe(df)
        
        # Performance optimization: Pre-populate caches
        logger.log_info("Pre-loading item and warehouse data...")
        unique_patterns = df['custom_item_name_detail'].dropna().unique().tolist()
        items_map = batch_find_items(unique_patterns)
        
        valid_item_codes = [item['item_code'] for item in items_map.values() if item is not None]
        warehouses_map = batch_get_warehouses(valid_item_codes)
        
        logger.log_info(f"Cached {len(items_map)} items and {len(warehouses_map)} warehouses")
        
        # Group theo custom_no
        grouped_data = df.groupby('custom_no')
        logger.log_info(f"T√¨m th·∫•y {len(grouped_data)} nh√≥m custom_no kh√°c nhau")
        
        # Performance optimization: Process groups in batches
        batch_size = 10  # Process 10 groups at a time
        groups_list = list(grouped_data)
        
        for batch_start in range(0, len(groups_list), batch_size):
            batch_end = min(batch_start + batch_size, len(groups_list))
            batch_groups = groups_list[batch_start:batch_end]
            
            logger.log_info(f"Processing batch {batch_start//batch_size + 1}: groups {batch_start+1}-{batch_end}")
            
            for group_index, (custom_no, group_df) in enumerate(batch_groups, batch_start + 1):
                try:
                    logger.log_info(f"X·ª≠ l√Ω nh√≥m {group_index}/{len(grouped_data)}: {custom_no} v·ªõi {len(group_df)} items")
                    
                    # T·∫°o Stock Entry cho nh√≥m n√†y
                    entry_result = create_stock_entry_for_group(custom_no, group_df)
                    
                    if entry_result["success"]:
                        result["success_count"] += 1
                        result["created_entries"].append(entry_result["stock_entry_name"])
                        result["created_entries_details"].append({
                            "name": entry_result["stock_entry_name"],
                            "posting_date": entry_result.get("posting_date", ""),
                            "custom_no": entry_result.get("custom_no", ""),
                            "custom_invoice_number": entry_result.get("custom_invoice_number", ""),
                            "items_count": entry_result["items_count"]
                        })
                        result["total_items"] += entry_result["items_count"]
                        logger.log_success(f"T·∫°o th√†nh c√¥ng: {entry_result['stock_entry_name']} v·ªõi {entry_result['items_count']} items")
                    else:
                        result["error_count"] += 1
                        error_msg = f"Nh√≥m {custom_no}: {entry_result['message']}"
                        result["errors"].append(error_msg)
                        logger.log_error(error_msg)
                        
                except Exception as e:
                    result["error_count"] += 1
                    error_msg = f"L·ªói x·ª≠ l√Ω nh√≥m {custom_no}: {str(e)}"
                    result["errors"].append(error_msg)
                    logger.log_error(error_msg)
                    continue
            
            # Performance optimization: Commit in batches
            if batch_end % 20 == 0:  # Commit every 2 batches (20 groups)
                frappe.db.commit()
                logger.log_info(f"Committed batch progress: {batch_end} groups processed")
        
        return result
        
    except Exception as e:
        result["success"] = False
        result["message"] = f"L·ªói x·ª≠ l√Ω file: {str(e)}"
        result["errors"].append(result["message"])
        logger.log_error(result["message"])
        return result

def validate_excel_columns(df):
    """
    Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc trong Excel
    """
    required_columns = [
        'custom_item_name_detail',  # ƒê·ªÉ t√¨m item
        'custom_no',            # ƒê·ªÉ group 
        'qty',                  # S·ªë l∆∞·ª£ng
        'custom_invoice_number' # S·ªë h√≥a ƒë∆°n
    ]
    
    # C√°c c·ªôt optional
    optional_columns = [
        'posting_date',         # Ng√†y ch·ª©ng t·ª´
        'posting_time',         # Th·ªùi gian ch·ª©ng t·ª´
        'custom_receive_date',  # Ng√†y nh·∫≠n h√†ng (specific for Material Receipt)
        't_warehouse'           # Target warehouse (optional - fallback to default if not provided)
    ]
    
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        return {
            "success": False,
            "message": f"Thi·∫øu c√°c c·ªôt b·∫Øt bu·ªôc: {missing_columns}"
        }
    
    # Log c√°c c·ªôt optional c√≥ s·∫µn
    available_optional = [col for col in optional_columns if col in df.columns]
    if available_optional:
        if logger:
            logger.log_info(f"C√°c c·ªôt optional c√≥ s·∫µn: {available_optional}")
    
    return {"success": True, "message": "C√°c c·ªôt h·ª£p l·ªá"}

def clean_dataframe(df):
    """
    L√†m s·∫°ch DataFrame
    """
    original_count = len(df)
    
    # X√≥a c√°c d√≤ng r·ªóng
    df = df.dropna(subset=['custom_no', 'custom_item_name_detail'])
    
    # Trim c√°c c·ªôt text
    text_columns = ['custom_no', 'custom_item_name_detail', 'custom_invoice_number']
    for col in text_columns:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()
    
    cleaned_count = len(df)
    removed_count = original_count - cleaned_count
    
    if removed_count > 0:
        logger.log_warning(f"ƒê√£ lo·∫°i b·ªè {removed_count} d√≤ng r·ªóng/kh√¥ng h·ª£p l·ªá")
    
    logger.log_success(f"Sau khi l√†m s·∫°ch c√≤n {cleaned_count} d√≤ng h·ª£p l·ªá")
    return df

def create_stock_entry_for_group(custom_no, group_df):
    """
    T·∫°o Stock Entry cho m·ªôt nh√≥m d·ªØ li·ªáu
    """
    try:
        # T·∫°o Stock Entry document
        stock_entry = create_stock_entry_doc(group_df.iloc[0], custom_no)
        
        # Debug: In ra posting fields tr∆∞·ªõc khi th√™m items
        logger.log_info(f"  üîç Before adding items:")
        logger.log_info(f"    - set_posting_time: {getattr(stock_entry, 'set_posting_time', 'NOT_SET')}")
        logger.log_info(f"    - posting_date: {getattr(stock_entry, 'posting_date', 'NOT_SET')}")
        logger.log_info(f"    - posting_time: {getattr(stock_entry, 'posting_time', 'NOT_SET')}")
        
        # Th√™m items v√†o Stock Entry
        items_added = 0
        item_errors = []
        
        for index, row in group_df.iterrows():
            item_result = add_item_to_stock_entry(stock_entry, row, index + 1)
            if item_result["success"]:
                items_added += 1
                logger.log_success(f"  Item {items_added}: {item_result['item_code']} - Qty: {row.get('qty', 0)}")
            else:
                item_errors.append(f"D√≤ng {index + 1}: {item_result['message']}")
                logger.log_error(f"  D√≤ng {index + 1}: {item_result['message']}")
        
        if items_added == 0:
            return {
                "success": False,
                "message": f"Kh√¥ng c√≥ item n√†o h·ª£p l·ªá. L·ªói: {'; '.join(item_errors)}"
            }
        
        # Set to_warehouse c·ªßa parent b·∫±ng t_warehouse c·ªßa item ƒë·∫ßu ti√™n (Material Receipt)
        if stock_entry.items and len(stock_entry.items) > 0:
            first_item = stock_entry.items[0]
            if hasattr(first_item, 't_warehouse') and first_item.t_warehouse:
                stock_entry.to_warehouse = first_item.t_warehouse
                logger.log_info(f"  Set to_warehouse: {first_item.t_warehouse}")
        
        # T·ªïng h·ª£p custom_invoice_number t·ª´ items (lo·∫°i b·ªè tr√πng)
        aggregate_invoice_numbers_to_parent(stock_entry)
        
        # Debug: In ra posting fields tr∆∞·ªõc khi save
        logger.log_info(f"  üîç Before save:")
        logger.log_info(f"    - set_posting_time: {getattr(stock_entry, 'set_posting_time', 'NOT_SET')}")
        logger.log_info(f"    - posting_date: {getattr(stock_entry, 'posting_date', 'NOT_SET')}")
        logger.log_info(f"    - posting_time: {getattr(stock_entry, 'posting_time', 'NOT_SET')}")
        
        # **QUAN TR·ªåNG**: Validate v√† set l·∫°i posting fields tr∆∞·ªõc khi save
        if not getattr(stock_entry, 'set_posting_time', None):
            stock_entry.set_posting_time = 1
            logger.log_warning(f"   Re-setting set_posting_time = 1")
        
        # ƒê·∫£m b·∫£o posting_date kh√¥ng r·ªóng
        if not getattr(stock_entry, 'posting_date', None):
            stock_entry.posting_date = nowdate()
            logger.log_warning(f"   Re-setting posting_date = {nowdate()}")
            
        # ƒê·∫£m b·∫£o posting_time kh√¥ng r·ªóng
        if not getattr(stock_entry, 'posting_time', None):
            stock_entry.posting_time = "17:00:00"
            logger.log_warning(f"   Re-setting posting_time = 17:00:00")
        
        # L∆∞u Stock Entry
        logger.log_info(f"  üíæ Saving Stock Entry...")
        stock_entry.save()
        logger.log_success(f"   Stock Entry saved: {stock_entry.name}")
        
        # Debug: Load l·∫°i document v√† ki·ªÉm tra posting fields sau khi save
        saved_doc = frappe.get_doc("Stock Entry", stock_entry.name)
        logger.log_info(f"  üîç After save (reloaded from DB):")
        logger.log_info(f"    - set_posting_time: {getattr(saved_doc, 'set_posting_time', 'NOT_SET')}")
        logger.log_info(f"    - posting_date: {getattr(saved_doc, 'posting_date', 'NOT_SET')}")
        logger.log_info(f"    - posting_time: {getattr(saved_doc, 'posting_time', 'NOT_SET')}")
        
        return {
            "success": True,
            "stock_entry_name": stock_entry.name,
            "items_count": items_added,
            "posting_date": getattr(stock_entry, 'posting_date', ''),
            "custom_no": getattr(stock_entry, 'custom_no', ''),
            "custom_invoice_number": getattr(stock_entry, 'custom_invoice_number', ''),
            "message": f"ƒê√£ t·∫°o th√†nh c√¥ng v·ªõi {items_added} items"
        }
        
    except Exception as e:
        logger.log_error(f"  üí• Exception details: {str(e)}")
        import traceback
        logger.log_error(f"  üìç Traceback: {traceback.format_exc()}")
        return {
            "success": False,
            "message": f"L·ªói t·∫°o Stock Entry: {str(e)}"
        }

def create_stock_entry_doc(first_row, custom_no):
    """
    T·∫°o Stock Entry document t·ª´ d√≤ng ƒë·∫ßu ti√™n
    """
    stock_entry = frappe.new_doc("Stock Entry")
    
    # Thi·∫øt l·∫≠p c√°c field c∆° b·∫£n
    stock_entry.company = DEFAULT_COMPANY
    stock_entry.purpose = "Material Receipt"
    stock_entry.stock_entry_type = "Material Receipt"
    
    # QUAN TR·ªåNG: Set set_posting_time = 1 ƒë·ªÉ c√≥ th·ªÉ t√πy ch·ªânh posting_date
    stock_entry.set_posting_time = 1
    
    # Thi·∫øt l·∫≠p posting_date
    posting_date_value = first_row.get('posting_date')
    final_posting_date = None

    if posting_date_value is not None and pd.notna(posting_date_value):
        try:
            # N·∫øu l√† datetime object t·ª´ pandas/Excel
            if hasattr(posting_date_value, 'strftime'):
                final_posting_date = posting_date_value.strftime('%Y-%m-%d')
                logger.log_info(f"  Processing posting_date from Excel datetime: {posting_date_value} -> {final_posting_date}")
            # N·∫øu l√† string
            elif isinstance(posting_date_value, str):
                final_posting_date = posting_date_value
                logger.log_info(f"  Processing posting_date from string: {posting_date_value}")
            else:
                # Convert v·ªÅ string n·∫øu c√≥ th·ªÉ
                final_posting_date = str(posting_date_value)
                logger.log_info(f"  Processing posting_date converted: {posting_date_value} -> {final_posting_date}")
        except Exception as e:
            logger.log_warning(f"L·ªói x·ª≠ l√Ω posting_date '{posting_date_value}': {str(e)}, s·ª≠ d·ª•ng ng√†y hi·ªán t·∫°i")
            final_posting_date = nowdate()
    else:
        final_posting_date = nowdate()
        logger.log_info(f"  Posting_date tr·ªëng, s·ª≠ d·ª•ng ng√†y hi·ªán t·∫°i: {final_posting_date}")
    
    # Set posting_date
    stock_entry.posting_date = final_posting_date
    logger.log_info(f"   Set posting_date = {final_posting_date}")
    
    # Thi·∫øt l·∫≠p posting_time
    posting_time_value = first_row.get('posting_time')
    final_posting_time = None
    
    if posting_time_value is not None and pd.notna(posting_time_value):
        try:
            # N·∫øu l√† time object
            if hasattr(posting_time_value, 'strftime'):
                final_posting_time = posting_time_value.strftime('%H:%M:%S')
                logger.log_info(f"  Processing posting_time from Excel time: {posting_time_value} -> {final_posting_time}")
            # N·∫øu l√† string
            elif isinstance(posting_time_value, str):
                final_posting_time = posting_time_value
                logger.log_info(f"  Processing posting_time from string: {posting_time_value}")
            else:
                final_posting_time = str(posting_time_value)
                logger.log_info(f"  Processing posting_time converted: {posting_time_value} -> {final_posting_time}")
        except Exception as e:
            logger.log_warning(f"L·ªói x·ª≠ l√Ω posting_time '{posting_time_value}': {str(e)}, s·ª≠ d·ª•ng th·ªùi gian m·∫∑c ƒë·ªãnh")
            final_posting_time = "08:00:00"
    else:
        # Set th·ªùi gian m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥ trong Excel
        final_posting_time = "08:00:00"
        logger.log_info(f"  Posting_time tr·ªëng, s·ª≠ d·ª•ng th·ªùi gian m·∫∑c ƒë·ªãnh: {final_posting_time}")
    
    # Set posting_time
    stock_entry.posting_time = final_posting_time
    logger.log_info(f"   Set posting_time = {final_posting_time}")
    
    # Debug: In ra c√°c field ƒë√£ set
    logger.log_info(f"  üîç Debug fields set:")
    logger.log_info(f"    - set_posting_time: {stock_entry.set_posting_time}")
    logger.log_info(f"    - posting_date: {stock_entry.posting_date}")
    logger.log_info(f"    - posting_time: {stock_entry.posting_time}")
    
    # Custom fields t·ª´ Excel (Material Receipt specific)
    custom_fields_mapping = {
        'custom_no': custom_no,
        'custom_note': 'custom_note'
    }
    
    for se_field, excel_field in custom_fields_mapping.items():
        if se_field == 'custom_no':
            # Custom_no lu√¥n set t·ª´ parameter
            setattr(stock_entry, se_field, custom_no)
            continue
            
        if excel_field in first_row and pd.notna(first_row[excel_field]):
            value = first_row[excel_field]
            
            # X·ª≠ l√Ω text fields - trim v√† format
            if isinstance(value, str):
                value = format_text_field(value, excel_field)
            
            setattr(stock_entry, se_field, value)
    
    return stock_entry

def format_text_field(value, field_name):
    """
    Format text field cho Material Receipt
    """
    if not value:
        return value
    
    # Trim v√† thay nhi·ªÅu kho·∫£ng tr·∫Øng b·∫±ng 1
    processed_value = str(value).strip()
    processed_value = ' '.join(processed_value.split())
    
    # Material Receipt doesn't need title case formatting for finished goods fields
    # Just return cleaned value
    return processed_value

# Performance optimization: Add item cache
item_cache = {}
warehouse_cache = {}

def find_item_by_pattern(pattern_search):
    """
    T√¨m item d·ª±a tr√™n exact match custom_item_name_detail v·ªõi caching
    """
    try:
        if not pattern_search or pd.isna(pattern_search):
            return None
            
        pattern_search = cstr(pattern_search).strip()
        
        # Check cache first
        if pattern_search in item_cache:
            return item_cache[pattern_search]
        
        # Exact match v·ªõi custom_item_name_detail
        items = frappe.get_list(
            "Item",
            filters={
                "custom_item_name_detail": pattern_search,
                "has_variants": 0,
            },
            fields=["name", "item_code", "item_name", "stock_uom", "custom_item_name_detail"],
            limit=1
        )
        
        if items:
            # Cache the result
            item_cache[pattern_search] = items[0]
            if logger:
                logger.log_info(f"   Found exact match: {items[0]['item_code']} - {items[0]['custom_item_name_detail']}")
            return items[0]
        
        # Cache negative result as well
        item_cache[pattern_search] = None
        if logger:
            logger.log_warning(f"  ‚ùå No exact match found for: '{pattern_search}'")
        
        return None
            
    except Exception as e:
        if logger:
            logger.log_error(f"L·ªói t√¨m item v·ªõi pattern '{pattern_search}': {str(e)}")
        return None

def batch_find_items(pattern_list):
    """
    Performance optimization: Batch lookup for multiple items
    """
    try:
        if not pattern_list:
            return {}
        
        # Filter out cached items
        uncached_patterns = [p for p in pattern_list if p not in item_cache]
        
        if uncached_patterns:
            # Batch query for all uncached items
            items = frappe.db.sql("""
                SELECT name, item_code, item_name, stock_uom, custom_item_name_detail
                FROM `tabItem`
                WHERE custom_item_name_detail IN %(patterns)s
                AND has_variants = 0
            """, {"patterns": uncached_patterns}, as_dict=True)
            
            # Update cache with results
            found_patterns = set()
            for item in items:
                pattern = item['custom_item_name_detail']
                item_cache[pattern] = item
                found_patterns.add(pattern)
            
            # Cache negative results
            for pattern in uncached_patterns:
                if pattern not in found_patterns:
                    item_cache[pattern] = None
        
        # Return all requested items from cache
        return {pattern: item_cache.get(pattern) for pattern in pattern_list}
        
    except Exception as e:
        if logger:
            logger.log_error(f"Error in batch item lookup: {str(e)}")
        return {}

def add_item_to_stock_entry(stock_entry, row, row_number):
    """
    Performance optimized item addition using cached lookups for Material Receipt
    """
    try:
        # Use cached item lookup
        pattern_search = row.get('custom_item_name_detail', '')
        item = item_cache.get(pattern_search)
        
        if not item:
            return {
                "success": False,
                "message": f"Kh√¥ng t√¨m th·∫•y item v·ªõi pattern: '{pattern_search}'"
            }
        
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng
        qty = flt(row.get('qty', 0))
        if qty <= 0:
            return {
                "success": False,
                "message": f"S·ªë l∆∞·ª£ng kh√¥ng h·ª£p l·ªá: {qty}"
            }
        
        # Determine warehouse - priority: Excel > Default warehouse
        warehouse = None
        excel_warehouse = row.get('t_warehouse', '')
        
        if excel_warehouse and pd.notna(excel_warehouse) and str(excel_warehouse).strip():
            # Use warehouse from Excel
            excel_warehouse = str(excel_warehouse).strip()
            if frappe.db.exists("Warehouse", excel_warehouse):
                warehouse = excel_warehouse
                logger.log_info(f"  Using warehouse from Excel: {warehouse} for item {item['item_code']}")
            else:
                return {
                    "success": False,
                    "message": f"Excel warehouse '{excel_warehouse}' kh√¥ng t·ªìn t·∫°i cho item {item['item_code']}"
                }
        else:
            # Fallback to default warehouse
            warehouse = warehouse_cache.get(item['item_code'])
            if warehouse:
                logger.log_info(f"  Using default warehouse: {warehouse} for item {item['item_code']}")
        
        if not warehouse:
            return {
                "success": False,
                "message": f"Kh√¥ng c√≥ warehouse n√†o ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh v√† kh√¥ng t√¨m th·∫•y default warehouse cho item {item['item_code']}"
            }
        
        # Ki·ªÉm tra invoice number
        invoice_number = cstr(row.get('custom_invoice_number', '')).strip()
        if not invoice_number:
            return {
                "success": False,
                "message": f"Thi·∫øu invoice number cho item {item['item_code']}"
            }
        
        # T·∫°o item row
        item_row = stock_entry.append("items", {})
        item_row.item_code = item['item_code']
        item_row.t_warehouse = warehouse  # Material Receipt uses t_warehouse
        item_row.qty = qty
        item_row.custom_invoice_number = invoice_number
        
        # Set custom_receive_date if available
        if 'custom_receive_date' in row and pd.notna(row['custom_receive_date']):
            try:
                receive_date = row['custom_receive_date']
                if hasattr(receive_date, 'strftime'):
                    item_row.custom_receive_date = receive_date.strftime('%Y-%m-%d')
                else:
                    item_row.custom_receive_date = str(receive_date)
            except Exception as e:
                logger.log_warning(f"Error setting receive_date: {str(e)}")
        
        # Material Receipt doesn't need finished goods fields sync
        
        return {
            "success": True,
            "item_code": item['item_code'],
            "message": f"Th√™m th√†nh c√¥ng item {item['item_code']}"
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"L·ªói th√™m item: {str(e)}"
        }

# Material Receipt doesn't need finished goods fields sync - removed sync_parent_fields_to_item

def aggregate_invoice_numbers_to_parent(stock_entry):
    """
    T·ªïng h·ª£p custom_invoice_number t·ª´ items v√†o parent (lo·∫°i b·ªè tr√πng)
    Gi·ªëng logic trong JS: aggregate_invoice_numbers()
    """
    try:
        if not stock_entry.items or len(stock_entry.items) == 0:
            return
        
        # Thu th·∫≠p t·∫•t c·∫£ invoice numbers t·ª´ items
        invoice_numbers = []
        
        for item in stock_entry.items:
            if hasattr(item, 'custom_invoice_number') and item.custom_invoice_number:
                invoice_num = cstr(item.custom_invoice_number).strip()
                # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ trong list (lo·∫°i b·ªè tr√πng)
                if invoice_num and invoice_num not in invoice_numbers:
                    invoice_numbers.append(invoice_num)
        
        # K·∫øt h·ª£p v·ªõi "; " separator
        aggregated_invoices = "; ".join(invoice_numbers)
        # Gi·ªõi h·∫°n t·ªëi ƒëa ƒë·ªô d√†i 140 k√Ω t·ª±
        if len(aggregated_invoices) > 140:
            aggregated_invoices = aggregated_invoices[:137] + "..."
            logger.log_warning(f"  Aggregated invoice numbers qu√° d√†i, c·∫Øt ng·∫Øn xu·ªëng 140 k√Ω t·ª±: {aggregated_invoices}")
        
        # Set v√†o parent field
        if aggregated_invoices:
            stock_entry.custom_invoice_number = aggregated_invoices
            logger.log_info(f"  Aggregated {len(invoice_numbers)} invoice numbers: {aggregated_invoices}")
        
    except Exception as e:
        logger.log_error(f"L·ªói t·ªïng h·ª£p invoice numbers: {str(e)}")

# Export c√°c h√†m ch√≠nh
__all__ = [
    'create_material_receipt',  
    'validate_excel_file',
    'import_material_receipt_from_excel',
    'get_default_warehouse_for_item'
]